# AIMCED
AI Music Composition and Emotion Detection System

## Overview
This project explores the fusion of artificial intelligence (AI), music composition, and emotion detection. Leveraging cutting-edge machine learning algorithms and techniques, the system is designed for innovative music generation and emotion analysis within the audio domain.

## Contents
1.Dataset
* Utilize a curated subset of the Lakh MIDI dataset for training.
  
2.Data Preprocessing
* Normalize MIDI data for AI model training.
* Extract musical elements from audio files.
* Generate music using LSTM networks.
* Detect emotions in audio using predefined criteria.
  
3.Results and Conclusions
* LSTM model captures short and long-range musical structures.
* Visualize network activations for interpretability.
* Identify future research areas.

## Getting Started

1.Dataset Setup
* Ensure the Lakh MIDI dataset subset is downloaded and accessible for training.

2.Environment Setup
* Create a virtual environment and install the necessary dependencies using the provided file.

3.Training the Model
* Run the data preprocessing and model training scripts to generate and save the AI model's weights.

4.Music Generation and Emotion Detection
* Utilize the trained model to generate music compositions and perform emotion detection on audio files.

## Contributions and Future Work
Contributions and feedback to enhance the project are welcome. Potential areas for future work include refining the model architecture, experimenting with different datasets, and exploring additional applications in the field of music technology.
